{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKPaDNL0f-m1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "from zipfile import ZipFile\n",
        "from urllib.request import urlretrieve\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import StringLookup\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the dataset"
      ],
      "metadata": {
        "id": "8DCAVcCZqSn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users = pd.read_csv(\"drive/MyDrive/RAW_interactions.csv\")\n",
        "recipes = pd.read_csv(\"drive/MyDrive/RAW_recipes.csv\")"
      ],
      "metadata": {
        "id": "2LdjUErOgfg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqjOEQSE3ed1",
        "outputId": "35a92724-38c6-4d85-cd81-14a7df1cee99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['user_id', 'recipe_id', 'date', 'rating', 'review'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recipes.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57P6Hh4b4P65",
        "outputId": "75954385-997c-434f-e14a-67f12f7a4057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['name', 'id', 'minutes', 'contributor_id', 'submitted', 'tags',\n",
              "       'nutrition', 'n_steps', 'steps', 'description', 'ingredients',\n",
              "       'n_ingredients'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the dataset"
      ],
      "metadata": {
        "id": "mGvjR57nqU8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users[\"user_id\"] = users[\"user_id\"].apply(lambda x: f\"user_{x}\")\n",
        "users[\"recipe_id\"] = users[\"recipe_id\"].apply(lambda x: f\"recipe_{x}\")\n",
        "users[\"rating\"] = users[\"rating\"].apply(lambda x: float(x))"
      ],
      "metadata": {
        "id": "l4pwDYD34bQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recipes[\"id\"] = recipes[\"id\"].apply(lambda x: f\"recipe_{x}\")\n",
        "recipes[\"minutes\"] = recipes[\"minutes\"].apply(lambda x: f\"recipe_{x}\")\n",
        "recipes[\"n_steps\"] = recipes[\"n_steps\"].apply(lambda x: f\"recipe_{x}\")\n",
        "recipes[\"n_ingredients\"] = recipes[\"n_ingredients\"].apply(lambda x: f\"recipe_{x}\")"
      ],
      "metadata": {
        "id": "dOdAJc9sEubB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treating the tags and the ingredients as one-hot encoded features for the model."
      ],
      "metadata": {
        "id": "4kU6kpurCOlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recipe_tags = []\n",
        "for val in recipes.tags:\n",
        "  for v in val.split(','):\n",
        "    recipe_tags.append(v)\n",
        "recipe_tags = set(recipe_tags)"
      ],
      "metadata": {
        "id": "vnC7XgIVF9r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tag in recipe_tags:\n",
        "    recipes[tag] = recipes[\"tags\"].apply(\n",
        "        lambda values: int(tag in values.split(\"|\"))\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cs2Q5GtqG2Qk",
        "outputId": "1726d404-cf94-4107-9571-2ec87d01ec42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-4d6eca7b214b>:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  recipes[tag] = recipes[\"tags\"].apply(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recipe_ig = []\n",
        "for val in recipes.ingredients:\n",
        "  for v in val.split(','):\n",
        "    recipe_ig.append(v)\n",
        "recipe_ig = set(recipe_ig)\n",
        "for ig in recipe_ig:\n",
        "    recipes[ig] = recipes[\"ingredients\"].apply(\n",
        "        lambda values: int(ig in values.split(\"|\"))\n",
        "    )"
      ],
      "metadata": {
        "id": "hI6H8KHSHgzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_group = users.sort_values(by=[\"date\"]).groupby(\"user_id\")\n",
        "\n",
        "ratings_data = pd.DataFrame(\n",
        "    data={\n",
        "        \"user_id\": list(ratings_group.groups.keys()),\n",
        "        \"recipe_ids\": list(ratings_group.recipe_id.apply(list)),\n",
        "        \"ratings\": list(ratings_group.rating.apply(list)),\n",
        "        \"timestamps\": list(ratings_group.date.apply(list))\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "UhkNWk60HGGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_data"
      ],
      "metadata": {
        "id": "chNa1084IFNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the recipes sequences per user (Each sequence being 4 recipe long)"
      ],
      "metadata": {
        "id": "YQIpS9aLqbuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 4\n",
        "step_size = 2\n",
        "\n",
        "\n",
        "def create_sequences(values, window_size, step_size):\n",
        "    sequences = []\n",
        "    start_index = 0\n",
        "    while True:\n",
        "        end_index = start_index + window_size\n",
        "        seq = values[start_index:end_index]\n",
        "        if len(seq) < window_size:\n",
        "            seq = values[-window_size:]\n",
        "            if len(seq) == window_size:\n",
        "                sequences.append(seq)\n",
        "            break\n",
        "        sequences.append(seq)\n",
        "        start_index += step_size\n",
        "    return sequences\n",
        "\n",
        "\n",
        "ratings_data.recipe_ids = ratings_data.recipe_ids.apply(\n",
        "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
        ")\n",
        "\n",
        "ratings_data.ratings = ratings_data.ratings.apply(\n",
        "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
        ")\n",
        "del ratings_data[\"timestamps\"]"
      ],
      "metadata": {
        "id": "f8asr6r1NobE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_data"
      ],
      "metadata": {
        "id": "w8EKZ0AoOsjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_data_movies = ratings_data[[\"user_id\", \"recipe_ids\"]].explode(\n",
        "    \"recipe_ids\", ignore_index=True\n",
        ")\n",
        "ratings_data_rating = ratings_data[[\"ratings\"]].explode(\"ratings\", ignore_index=True)\n",
        "ratings_data_transformed = pd.concat([ratings_data_movies, ratings_data_rating], axis=1)\n",
        "ratings_data_transformed = ratings_data_transformed[ratings_data_transformed['recipe_ids'].notna()]\n",
        "# ratings_data_transformed = ratings_data_transformed.join(\n",
        "#     recipes_n.set_index(\"recipe_ids\"), on=\"recipe_ids\"\n",
        ")\n",
        "# ratings_data_transformed.recipe_ids = ratings_data_transformed.recipe_ids.apply(\n",
        "#     lambda x: \",\".join(x)\n",
        "# )\n",
        "\n",
        "ratings_data_transformed.ratings = ratings_data_transformed.ratings.apply(\n",
        "    lambda x: \",\".join([str(v) for v in x])\n",
        ")\n",
        "\n",
        "\n",
        "ratings_data_transformed.rename(\n",
        "    columns={\"recipe_ids\": \"sequence_recipe_ids\", \"ratings\": \"sequence_ratings\"},\n",
        "    inplace=True,\n",
        ")"
      ],
      "metadata": {
        "id": "JlG_0QOvRxFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_data_transformed"
      ],
      "metadata": {
        "id": "ADr7IGRgSlC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dividing to train and test set"
      ],
      "metadata": {
        "id": "PhpjP2VhCffU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_selection = np.random.rand(len(ratings_data_transformed.index)) <= 0.85\n",
        "train_data = ratings_data_transformed[random_selection]\n",
        "test_data = ratings_data_transformed[~random_selection]\n",
        "\n",
        "train_data.to_csv(\"train_data.csv\", index=False, sep=\"|\", header=False)\n",
        "test_data.to_csv(\"test_data.csv\", index=False, sep=\"|\", header=False)"
      ],
      "metadata": {
        "id": "-H149vznT-Vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CSV_HEADER = list(ratings_data_transformed.columns)\n",
        "\n",
        "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
        "    \"user_id\": list(users.user_id.unique()),\n",
        "    \"recipe_id\": list(users.recipe_id.unique()),\n",
        "    # \"minutes\": list(recipes.minutes.unique()),\n",
        "    # \"n_steps\": list(recipes.n_steps.unique()),\n",
        "    # \"n_ingredients\": list(recipes.n_ingredients.unique()),\n",
        "}\n",
        "\n",
        "# USER_FEATURES = [\"sex\", \"age_group\", \"occupation\"]\n",
        "\n",
        "# RECIPE_FEATURES = [\"minutes\",\"n_steps\",\"n_ingredients\"]"
      ],
      "metadata": {
        "id": "TnZNlDN1Uo1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intializing the model"
      ],
      "metadata": {
        "id": "FwClw63mqfE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n",
        "    def process(features):\n",
        "        recipe_ids_string = features[\"sequence_recipe_ids\"]\n",
        "        sequence_recipe_ids = tf.strings.split(recipe_ids_string, \",\").to_tensor()\n",
        "\n",
        "        # The last movie id in the sequence is the target movie.\n",
        "        features[\"target_recipe_id\"] = sequence_recipe_ids[:, -1]\n",
        "        features[\"sequence_recipe_ids\"] = sequence_recipe_ids[:, :-1]\n",
        "\n",
        "        ratings_string = features[\"sequence_ratings\"]\n",
        "        sequence_ratings = tf.strings.to_number(\n",
        "            tf.strings.split(ratings_string, \",\"), tf.dtypes.float32\n",
        "        ).to_tensor()\n",
        "\n",
        "        # The last rating in the sequence is the target for the model to predict.\n",
        "        target = sequence_ratings[:, -1]\n",
        "        features[\"sequence_ratings\"] = sequence_ratings[:, :-1]\n",
        "\n",
        "        return features, target\n",
        "\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        csv_file_path,\n",
        "        batch_size=batch_size,\n",
        "        column_names=CSV_HEADER,\n",
        "        num_epochs=1,\n",
        "        header=False,\n",
        "        field_delim=\"|\",\n",
        "        shuffle=shuffle,\n",
        "    ).map(process)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "S2i4fSj4YjgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_inputs():\n",
        "    return {\n",
        "        \"user_id\": layers.Input(name=\"user_id\", shape=(1,), dtype=tf.string),\n",
        "        \"sequence_recipe_ids\": layers.Input(\n",
        "            name=\"sequence_recipe_ids\", shape=(sequence_length - 1,), dtype=tf.string\n",
        "        ),\n",
        "        \"target_recipe_id\": layers.Input(\n",
        "            name=\"target_recipe_id\", shape=(1,), dtype=tf.string\n",
        "        ),\n",
        "        \"sequence_ratings\": layers.Input(\n",
        "            name=\"sequence_ratings\", shape=(sequence_length - 1,), dtype=tf.float32\n",
        "        ),\n",
        "        # \"minutes\": layers.Input(name=\"minutes\", shape=(1,), dtype=tf.string),\n",
        "        # \"n_steps\": layers.Input(name=\"n_steps\", shape=(1,), dtype=tf.string),\n",
        "        # \"n_ingredients\": layers.Input(name=\"n_ingredients\", shape=(1,), dtype=tf.string)\n",
        "    }"
      ],
      "metadata": {
        "id": "4XoUQvIsY4v6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding the input features for the recipe"
      ],
      "metadata": {
        "id": "YjZw1TCOCiLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_input_features(\n",
        "    inputs,\n",
        "    include_user_id=True,\n",
        "    include_user_features=False,\n",
        "    include_recipe_features=True,\n",
        "):\n",
        "\n",
        "    encoded_transformer_features = []\n",
        "    encoded_other_features = []\n",
        "\n",
        "    other_feature_names = []\n",
        "    if include_user_id:\n",
        "        other_feature_names.append(\"user_id\")\n",
        "    # if not include_recipe_features:\n",
        "    #     other_feature_names.extend(RECIPE_FEATURES)\n",
        "\n",
        "    ## Encode user features\n",
        "    print(other_feature_names)\n",
        "    for feature_name in other_feature_names:\n",
        "        # Convert the string input values into integer indices.\n",
        "        vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
        "        idx = StringLookup(vocabulary=vocabulary, mask_token=None, num_oov_indices=1)(\n",
        "            inputs[feature_name]\n",
        "        )\n",
        "        # Compute embedding dimensions\n",
        "        embedding_dims = int(math.sqrt(len(vocabulary)))\n",
        "        # Create an embedding layer with the specified dimensions.\n",
        "        embedding_encoder = layers.Embedding(\n",
        "            input_dim=len(vocabulary),\n",
        "            output_dim=embedding_dims,\n",
        "            name=f\"{feature_name}_embedding\",\n",
        "        )\n",
        "        # Convert the index values to embedding representations.\n",
        "        encoded_other_features.append(embedding_encoder(idx))\n",
        "\n",
        "    ## Create a single embedding vector for the user features\n",
        "    if len(encoded_other_features) > 1:\n",
        "        encoded_other_features = layers.concatenate(encoded_other_features)\n",
        "    elif len(encoded_other_features) == 1:\n",
        "        encoded_other_features = encoded_other_features[0]\n",
        "    else:\n",
        "        encoded_other_features = None\n",
        "\n",
        "    ## Create a recipe embedding encoder\n",
        "    recipe_vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[\"recipe_id\"]\n",
        "    recipe_embedding_dims = int(math.sqrt(len(recipe_vocabulary)))\n",
        "    # Create a lookup to convert string values to integer indices.\n",
        "    recipe_index_lookup = StringLookup(\n",
        "        vocabulary=recipe_vocabulary,\n",
        "        mask_token=None,\n",
        "        num_oov_indices=1,\n",
        "        name=\"recipe_index_lookup\",\n",
        "    )\n",
        "    # Create an embedding layer with the specified dimensions.\n",
        "    recipe_embedding_encoder = layers.Embedding(\n",
        "        input_dim=len(recipe_vocabulary),\n",
        "        output_dim=recipe_embedding_dims,\n",
        "        name=f\"recipe_embedding\",\n",
        "    )\n",
        "    # Create a vector lookup for movie genres.\n",
        "    tag_vectors = recipes[recipe_tags].to_numpy()\n",
        "    recipe_tags_lookup = layers.Embedding(\n",
        "        input_dim=tag_vectors.shape[0],\n",
        "        output_dim=tag_vectors.shape[1],\n",
        "        embeddings_initializer=tf.keras.initializers.Constant(tag_vectors),\n",
        "        trainable=False,\n",
        "        name=\"tags_vector\",\n",
        "    )\n",
        "\n",
        "    ig_vectors = recipes[recipe_ig].to_numpy()\n",
        "    recipe_ig_lookup = layers.Embedding(\n",
        "        input_dim=ig_vectors.shape[0],\n",
        "        output_dim=ig_vectors.shape[1],\n",
        "        embeddings_initializer=tf.keras.initializers.Constant(ig_vectors),\n",
        "        trainable=False,\n",
        "        name=\"ig_vector\",\n",
        "    )\n",
        "    # Create a processing layer for genres.\n",
        "    recipe_embedding_processor = layers.Dense(\n",
        "        units=recipe_embedding_dims,\n",
        "        activation=\"relu\",\n",
        "        name=\"process_recipe_embedding_with_tags\",\n",
        "    )\n",
        "\n",
        "\n",
        "    ## Define a function to encode a given movie id.\n",
        "    def encode_recipe(recipe_id):\n",
        "        # Convert the string input values into integer indices.\n",
        "        recipe_idx = recipe_index_lookup(recipe_id)\n",
        "        recipe_embedding = recipe_embedding_encoder(recipe_idx)\n",
        "        encoded_recipe = recipe_embedding\n",
        "        if include_recipe_features:\n",
        "            recipe_tags_vector = recipe_tags_lookup(recipe_idx)\n",
        "            recipe_ig_vector = recipe_ig_lookup(recipe_idx)\n",
        "            encoded_recipe = recipe_embedding_processor(\n",
        "                layers.concatenate([recipe_embedding, recipe_tags_vector,recipe_ig_vector])\n",
        "            )\n",
        "        return encoded_recipe\n",
        "\n",
        "    ## Encoding target_movie_id\n",
        "    target_recipe_id = inputs[\"target_recipe_id\"]\n",
        "    encoded_target_recipe = encode_recipe(target_recipe_id)\n",
        "\n",
        "    ## Encoding sequence movie_ids.\n",
        "    sequence_recipes_ids = inputs[\"sequence_recipe_ids\"]\n",
        "    encoded_sequence_recipes = encode_recipe(sequence_recipes_ids)\n",
        "\n",
        "   \n",
        "\n",
        "    # Create positional embedding.\n",
        "    position_embedding_encoder = layers.Embedding(\n",
        "        input_dim=sequence_length,\n",
        "        output_dim=recipe_embedding_dims,\n",
        "        name=\"position_embedding\",\n",
        "    )\n",
        "    positions = tf.range(start=0, limit=sequence_length - 1, delta=1)\n",
        "    encodded_positions = position_embedding_encoder(positions)\n",
        "    # Retrieve sequence ratings to incorporate them into the encoding of the movie.\n",
        "    sequence_ratings = tf.expand_dims(inputs[\"sequence_ratings\"], -1)\n",
        "    # Add the positional encoding to the movie encodings and multiply them by rating.\n",
        "    encoded_sequence_recipes_with_poistion_and_rating = layers.Multiply()(\n",
        "        [(encoded_sequence_recipes + encodded_positions), sequence_ratings,]\n",
        "    )\n",
        "\n",
        "    # Construct the transformer inputs.\n",
        "    for encoded_recipe in tf.unstack(\n",
        "        encoded_sequence_recipes_with_poistion_and_rating, axis=1\n",
        "    ):\n",
        "        encoded_transformer_features.append(tf.expand_dims(encoded_recipe, 1))\n",
        "    encoded_transformer_features.append(encoded_target_recipe)\n",
        "\n",
        "    encoded_transformer_features = layers.concatenate(\n",
        "        encoded_transformer_features, axis=1\n",
        "    )\n",
        "\n",
        "    return encoded_transformer_features, encoded_other_features"
      ],
      "metadata": {
        "id": "TAWww4qBZif-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the model"
      ],
      "metadata": {
        "id": "noFHMgRUCnvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "include_user_id = False\n",
        "include_user_features = False\n",
        "include_recipe_features = False\n",
        "\n",
        "hidden_units = [256, 128]\n",
        "dropout_rate = 0.1\n",
        "num_heads = 3\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    inputs = create_model_inputs()\n",
        "    print(inputs)\n",
        "    transformer_features, other_features = encode_input_features(\n",
        "        inputs, include_user_id, include_user_features, include_recipe_features\n",
        "    )\n",
        "    print(transformer_features)\n",
        "\n",
        "    # Create a multi-headed attention layer.\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=transformer_features.shape[2], dropout=dropout_rate\n",
        "    )(transformer_features, transformer_features)\n",
        "\n",
        "    # Transformer block.\n",
        "    attention_output = layers.Dropout(dropout_rate)(attention_output)\n",
        "    x1 = layers.Add()([transformer_features, attention_output])\n",
        "    x1 = layers.LayerNormalization()(x1)\n",
        "    x2 = layers.LeakyReLU()(x1)\n",
        "    x2 = layers.Dense(units=x2.shape[-1])(x2)\n",
        "    x2 = layers.Dropout(dropout_rate)(x2)\n",
        "    transformer_features = layers.Add()([x1, x2])\n",
        "    transformer_features = layers.LayerNormalization()(transformer_features)\n",
        "    features = layers.Flatten()(transformer_features)\n",
        "\n",
        "    # Included the other features.\n",
        "    if other_features is not None:\n",
        "        features = layers.concatenate(\n",
        "            [features, layers.Reshape([other_features.shape[-1]])(other_features)]\n",
        "        )\n",
        "\n",
        "    # Fully-connected layers.\n",
        "    for num_units in hidden_units:\n",
        "        features = layers.Dense(num_units)(features)\n",
        "        features = layers.BatchNormalization()(features)\n",
        "        features = layers.LeakyReLU()(features)\n",
        "        features = layers.Dropout(dropout_rate)(features)\n",
        "\n",
        "    outputs = layers.Dense(units=1)(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "model = create_model()"
      ],
      "metadata": {
        "id": "PdzKjWAoeA0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the metrics - NDCG and MRR and loss function - MAE for training the model"
      ],
      "metadata": {
        "id": "h-__XeLzqiBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow_ranking as tfr\n",
        "from keras.metrics import top_k_categorical_accuracy\n",
        "\n",
        "eval_metrics = [\n",
        "    tfr.keras.metrics.get(key=\"ndcg\", name=\"metric/ndcg\", ragged=False),\n",
        "    tfr.keras.metrics.get(key=\"mrr\", name=\"metric/mrr\", ragged=False)\n",
        "]\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adagrad(learning_rate=0.01),\n",
        "    loss=keras.losses.MeanSquaredError(),\n",
        "    metrics=[keras.metrics.MeanAbsoluteError(),'top_k_categorical_accuracy',eval_metrics],\n",
        ")\n",
        "\n",
        "# Read the training data.\n",
        "train_dataset = get_dataset_from_csv(\"train_data.csv\", shuffle=True, batch_size=265)\n",
        "\n",
        "# Fit the model with the training data.\n",
        "model.fit(train_dataset, epochs=5)\n",
        "\n",
        "# Read the test data.\n",
        "test_dataset = get_dataset_from_csv(\"test_data.csv\", batch_size=265)\n",
        "\n",
        "# Evaluate the model on the test data.\n",
        "_, rmse = model.evaluate(test_dataset, verbose=0)\n",
        "print(f\"Test MAE: {round(rmse, 3)}\")\n"
      ],
      "metadata": {
        "id": "w8iOdaFzeFzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the test set"
      ],
      "metadata": {
        "id": "dE4FKWM7l0iY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_dataset, verbose=0)\n",
        "\n",
        "# [0.9269737601280212,\n",
        "#  0.5811803936958313,\n",
        "#  0.9725525379180908 - ndcg,\n",
        "#  0.9725525379180908]"
      ],
      "metadata": {
        "id": "aDO-nlWCu12K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}